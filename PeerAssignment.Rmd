---
title: "Predicting Weight Lifting Exercises using Human Activity Recognition (HAR)"
author: "Kristina Dill"
date: "Sunday, September 21, 2014"
output:
  html_document:
    fig_capion: yes
    highlight: tango
    theme: united
  pdf_document: default
---

### Synopsis 
The [HAR Weight Lifting data](http://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html) focused on discriminating between different weight lifting exercises to investigate how an activity was performed. Participants performed Unilateral Dumbbell Biceps Curl in five different fashions:

*   Class A:  Exactly according to the specification 
*   Class B:  Throwing the elbows to the front 
*   Class C:  Lifting the dumbbell only halfway 
*   Class D:  Lowering the dumbbell only halfway 
*   Class E:  Throwing the hips to the front

In this project, 159 variables were used to predict the manner in which they did the exercise using the "classe" variable in the training set. The training dataset was preprocessed and the final model was built using the random forest method and cross-validation to reduce the overfitting. The final model was used to predict 20 different test cases.

### Data Processing

The following R libraries were required to analysis HAR weight lifting exercise data. 
```{r Load libraries}
library(plyr, warn.conflicts = FALSE,quietly=TRUE)
library(caret, warn.conflicts = FALSE,quietly=TRUE)
library(Hmisc, warn.conflicts = FALSE,quietly=TRUE)
library(AppliedPredictiveModeling, warn.conflicts = FALSE,quietly=TRUE)
library(xtable, warn.conflicts = FALSE,quietly=TRUE)
library(randomForest, warn.conflicts = FALSE,quietly=TRUE)
```

The weight lifting HAR training data were downloaded and loaded into R using the download.file and read.csv functions. In order to predict the classe type of weight lifting, this dataset was split into two parts: preliminary training and test sets. The seed was set to 975 for this analysis. 

The preliminary training dataset was pre-processed before any analysis was performed. Predictors with a large portion of missing observations and near-zero variances were removed. This reduced the number of predictors from 159 to 58. 

```{r downloading data, cache=TRUE, results='asis'}
download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", "weightTraining.csv")
WeightTrain <- read.csv("weightTraining.csv")
```

```{r preprocessing I, cache=TRUE, results='asis'}
set.seed(975)
inTrain = createDataPartition(WeightTrain$classe, p = 3/4)[[1]]
training = WeightTrain[ inTrain,]
testing = WeightTrain[-inTrain,]
 
# Remove NAs variables 
MissingVars <- data.frame(apply(training,2,function(x) sum(is.na(x))))
colnames(MissingVars) <- c("NACounts")
VarsNotNA <-rownames(MissingVars)[MissingVars$NACounts==0]
TrainNoMissing <- training[,colnames(training) %in%VarsNotNA]

# Remove near Zero Variance variables
nsv <- nearZeroVar(TrainNoMissing, saveMetrics=TRUE)
nsv_KeyCols <- rownames(nsv)[nsv$nzv==FALSE]
newTrain <- TrainNoMissing[,colnames(TrainNoMissing) %in% nsv_KeyCols]
```

After preprocessing the preliminary training dataset, the training model was built using the random forest method and 10-fold cross validation. The random forest method was used to improve the accuracy of the model. 
However, in random forest models overfitting is an issue. A 10-fold cross validation was used to reduce the bias and minmize overfitting. The random forests were tuned to include 250 trees in each cross validation iteration.

```{r Model, cache=TRUE, results='asis'}
modelFit1 <-train(newTrain$classe~.,method="rf", ntree=250, trControl=trainControl(method="cv", number=10), data=newTrain[,-c(1:6,59)])
```

In Figure 1, the error rates of the final model are plotted over the final 250 trees. The Error rates converge as the number of tree approach 250. For all 5 classe the error rates are less than 0.05. 

```{r RandomForest Error Rates, fig.height=5, fig.width=7}
plot(modelFit1$finalModel, main= "Random Forest Final Model Error Rates")
```

###### **Figure 1**: Error Rates of Final Random Forest Model.

In the following figure (Figure 2), the importance of the predictor variables in the final model are plotted. The Mean decrease Gini index is used to determine the most important variables.    

```{r VarImportaince, fig.height=7, fig.width=6}
varImpPlot(modelFit1$finalModel, main="Variable Importance: Final \nRandom Forest Model")
```

###### **Figure 2**: Variable Importance in the Final Random Forest Model.

The top 10 most important variables are: 
``` {r Importance Vars, results ='asis'}
VarImp <- importance(modelFit1$finalModel)
print(xtable(head(VarImp,10)), type="html")
```

The final random forest model was used to predict the outcome of the preliminary test weight lifting dataset.

```{r ConfusionMatrix, results= 'asis'}
ConMatrix <-confusionMatrix(testing$classe,predict(modelFit1,testing))
print(ConMatrix$overall)
```

The final model prediction accuracy is 99.16%.  

### Results
Now that the final model is assessed, this model is used on the actual testing dataset of 20 participants. 
The weight lifting HAR test data set was downloaded and loaded into R using the download.file and read.csv functions.

```{r actual Test, cache=TRUE, results= 'asis'}
download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", "WeightTest.csv")
FinalTest <-read.csv("WeightTest.csv")
TestPred <- data.frame(predict(modelFit1,FinalTest))
colnames(TestPred) <- "Model_Prediction"

print(xtable(TestPred), type="html")
```

### Conclusion 

In conclusion, The weight lifting training dataset was used to create a model that predicted the the way a participant performed the weight lifting exercise. After the final model was built and the perfomance assessed (accuracy 99%) on a preliminary test set, the final model was used to predict 20 participants weight lifting class. 
